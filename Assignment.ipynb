{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT-1:\n",
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 2. Load Dataset\n",
    "# Example dataset (you can replace with your CSV file)\n",
    "# Let's create a small height-weight dataset for demonstration\n",
    "data = {\n",
    "    \"Height\": [150, 152, 155, 160, 165, 170, 172, 175, 180, 185],\n",
    "    \"Weight\": [50, 52, 53, 55, 65, 70, 72, 75, 78, 85]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 3. Explore Dataset\n",
    "print(\"First five rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "\n",
    "# 4. Visualize Data\n",
    "plt.scatter(df[\"Height\"], df[\"Weight\"])\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Weight (kg)\")\n",
    "plt.title(\"Height vs Weight Scatter Plot\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Split Data\n",
    "X = df[[\"Height\"]]   # independent variable\n",
    "y = df[\"Weight\"]     # dependent variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 6. Train Model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 7. Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 8. Evaluate Model\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# Print model coefficients\n",
    "print(\"Intercept (b0):\", model.intercept_)\n",
    "print(\"Slope (b1):\", model.coef_[0])\n",
    "\n",
    "# 9. Plot Regression Line\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, model.predict(X), color='red')  # best-fit line\n",
    "plt.xlabel(\"Height (cm)\")\n",
    "plt.ylabel(\"Weight (kg)\")\n",
    "plt.title(\"Linear Regression Line\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT-2:\n",
    "\n",
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 2. Load Dataset\n",
    "# Here we use the Boston Housing dataset from sklearn / openml (commonly used on Kaggle).\n",
    "from sklearn.datasets import fetch_openml\n",
    "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
    "\n",
    "df = boston.frame  # DataFrame including features + target\n",
    "df.columns = list(boston.feature_names) + [\"PRICE\"]  # rename target column\n",
    "\n",
    "# 3. Explore Dataset\n",
    "print(\"Shape:\", df.shape)\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "\n",
    "# 4. (Optional) Visualize relationships - pairplot or correlation heatmap\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(df.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation matrix\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Choose features (>= 5) and target\n",
    "# E.g., pick 5–7 features that likely influence price\n",
    "features = [\"RM\", \"LSTAT\", \"PTRATIO\", \"INDUS\", \"NOX\", \"AGE\", \"DIS\"]\n",
    "X = df[features]\n",
    "y = df[\"PRICE\"]\n",
    "\n",
    "# 6. Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 7. Train Multiple Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 8. Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 9. Report results: coefficients, intercept, MSE, R²\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "coeff_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Coefficient\": model.coef_\n",
    "})\n",
    "print(coeff_df)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R² Score:\", r2)\n",
    "\n",
    "# 10. Plot Actual vs Predicted\n",
    "plt.scatter(y_test, y_pred, color='blue')\n",
    "plt.plot([y_test.min(), y_test.max()],\n",
    "         [y_test.min(), y_test.max()],\n",
    "         color='red', lw=2)\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Actual vs Predicted House Price\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT-3:\n",
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 2. Load Dataset (Iris)\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['Species'] = data.target\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# 3. Features and Target\n",
    "X = df.drop(\"Species\", axis=1)\n",
    "y = df[\"Species\"]\n",
    "\n",
    "# 4. Train-Test Split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 5. Train Model 1: Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "\n",
    "# 6. Train Model 2: Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# 7. Compare Accuracy\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "\n",
    "# 8. Detailed Evaluation\n",
    "print(\"\\nClassification Report (Logistic Regression):\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\nClassification Report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e532046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT-4:\n",
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# 2. Load UCI Banknote Dataset\n",
    "# You can download from: https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\n",
    "\n",
    "df = pd.read_csv(url, header=None)\n",
    "df.columns = [\"variance\",\"skewness\",\"curtosis\",\"entropy\",\"class\"]\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# 3. Features and Target\n",
    "X = df.drop(\"class\", axis=1)\n",
    "y = df[\"class\"]\n",
    "\n",
    "# 4. Train-Test Split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 5. Logistic Regression Model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "y_prob_lr = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 6. Decision Tree Model\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "y_prob_tree = tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 7. Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "cm_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "\n",
    "print(\"Confusion Matrix (Logistic Regression):\\n\", cm_lr)\n",
    "print(\"\\nConfusion Matrix (Decision Tree):\\n\", cm_tree)\n",
    "\n",
    "# 8. ROC and AUC\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)\n",
    "auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "fpr_tree, tpr_tree, _ = roc_curve(y_test, y_prob_tree)\n",
    "auc_tree = auc(fpr_tree, tpr_tree)\n",
    "\n",
    "print(\"\\nAUC (Logistic Regression):\", auc_lr)\n",
    "print(\"AUC (Decision Tree):\", auc_tree)\n",
    "\n",
    "# 9. Plot ROC Curves\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_lr, tpr_lr, label=f\"Logistic Regression (AUC = {auc_lr:.2f})\")\n",
    "plt.plot(fpr_tree, tpr_tree, label=f\"Decision Tree (AUC = {auc_tree:.2f})\")\n",
    "plt.plot([0,1], [0,1], \"k--\")  # baseline\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 10. Classification Reports\n",
    "print(\"\\nLogistic Regression Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\nDecision Tree Report:\")\n",
    "print(classification_report(y_test, y_pred_tree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce0ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT-5:\n",
    "\n",
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 2. Load Wine Dataset\n",
    "wine = load_wine()\n",
    "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "y = pd.Series(wine.target, name=\"Class\")\n",
    "\n",
    "print(\"Class Distribution Before SMOTE:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# 3. Train-Test Split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 4. Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Handle Imbalance using SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nClass Distribution After SMOTE:\")\n",
    "print(y_train_res.value_counts())\n",
    "\n",
    "# 6. Model 1: Logistic Regression (Multiclass)\n",
    "log_reg = LogisticRegression(max_iter=300, multi_class='multinomial')\n",
    "log_reg.fit(X_train_res, y_train_res)\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# 7. Model 2: Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_res, y_train_res)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# 8. Evaluation Metrics\n",
    "\n",
    "print(\"\\n================ LOGISTIC REGRESSION ================\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\n================ RANDOM FOREST ======================\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Confusion matrices\n",
    "print(\"\\nConfusion Matrix (Logistic Regression):\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\nConfusion Matrix (Random Forest):\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d0abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT-6:\n",
    "\n",
    "\n",
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 2. Load Dataset (Mall Customers)\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/mall_customers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# 3. Select Features for Clustering\n",
    "X = df[[\"Annual_Income\", \"Spending_Score\"]]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Elbow Method\n",
    "# ----------------------------\n",
    "inertia_vals = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    model.fit(X_scaled)\n",
    "    inertia_vals.append(model.inertia_)\n",
    "\n",
    "# Plot Elbow Curve\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(K_range, inertia_vals, marker='o')\n",
    "plt.title(\"Elbow Method - Optimal K\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Silhouette Score Method\n",
    "# ----------------------------\n",
    "silhouette_vals = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    score = silhouette_score(X_scaled, labels)\n",
    "    silhouette_vals.append(score)\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(K_range, silhouette_vals, marker='o')\n",
    "plt.title(\"Silhouette Score Method\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.show()\n",
    "\n",
    "# Choose optimal K (usually 5)\n",
    "k_optimal = 5\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df[\"Cluster\"] = clusters\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Visualize Clusters\n",
    "# ----------------------------\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(df[\"Annual_Income\"], df[\"Spending_Score\"],\n",
    "            c=df[\"Cluster\"], cmap=\"viridis\", s=60)\n",
    "\n",
    "centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centers[:,0], centers[:,1], \n",
    "            c='red', s=200, marker='X', label=\"Centroids\")\n",
    "\n",
    "plt.title(\"Customer Segmentation using K-Means\")\n",
    "plt.xlabel(\"Annual Income\")\n",
    "plt.ylabel(\"Spending Score\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57772dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT-7:\n",
    "\n",
    "\n",
    "# 1. Import Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 2. Load High-Dimensional Dataset\n",
    "digits = load_digits()\n",
    "X = digits.data  # 64 features\n",
    "y = digits.target\n",
    "\n",
    "print(\"Original Shape:\", X.shape)\n",
    "\n",
    "# 3. Standardize Data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 4. PCA: Reduce to 2 components (for visualization)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Shape After PCA:\", X_pca.shape)\n",
    "\n",
    "# 5. Apply K-Means\n",
    "k = 10  # digits dataset → 10 clusters expected\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# 6. Evaluate Clustering Quality\n",
    "inertia = kmeans.inertia_\n",
    "sil_score = silhouette_score(X_pca, clusters)\n",
    "\n",
    "print(\"Inertia:\", inertia)\n",
    "print(\"Silhouette Score:\", sil_score)\n",
    "\n",
    "# 7. Visualization of PCA Clusters\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap=\"tab10\", s=20)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "            color='red', marker='X', s=200, label='Centroids')\n",
    "plt.title(\"PCA + K-Means Clustering\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT-8:\n",
    "\n",
    "\n",
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 2. Load Dataset\n",
    "url = \"https://raw.githubusercontent.com/plotly/datasets/master/creditcard.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# 3. Feature Scaling for 'Amount'\n",
    "scaler = StandardScaler()\n",
    "df['Amount'] = scaler.fit_transform(df[['Amount']])\n",
    "\n",
    "# Drop Time column (not useful)\n",
    "df = df.drop('Time', axis=1)\n",
    "\n",
    "# 4. Features\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# 5. Isolation Forest Model\n",
    "iso = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=0.01,   # approx fraud 1%\n",
    "    random_state=42\n",
    ")\n",
    "iso.fit(X)\n",
    "\n",
    "# 6. Predictions\n",
    "y_pred = iso.predict(X)\n",
    "# IsolationForest returns:\n",
    "# 1 = normal, -1 = anomaly → convert to 0/1\n",
    "y_pred = np.where(y_pred == -1, 1, 0)\n",
    "\n",
    "# 7. Evaluation\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y, y_pred))\n",
    "\n",
    "# 8. Visualization of Fraud vs Normal\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.countplot(x=y_pred)\n",
    "plt.title(\"Detected Anomalies by Isolation Forest\")\n",
    "plt.xlabel(\"Predicted Class (0=Normal, 1=Fraud)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 9. 2D Visualization (Amount vs PCA component V2)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(df['V2'], df['Amount'], c=y_pred, cmap='coolwarm', s=5)\n",
    "plt.title(\"Fraud Detection – Isolation Forest\")\n",
    "plt.xlabel(\"V2\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
